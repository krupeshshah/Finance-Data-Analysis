{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "    \"student-1\": { \"id\": \"1\", \"name\": \"sravan\",\"age\":22 },\n",
      "    \"student-2\":{ \"id\": \"2\", \"name\": \"harsha\",\"age\":22 },\n",
      "    \"student-3\": { \"id\": \"3\", \"name\": \"deepika\",\"age\":21 },\n",
      "    \"student-4\": { \"id\": \"4\", \"name\": \"jyothika\",\"age\":23 }\n",
      "} \n",
      "           id      name  age\n",
      "student-1   1    sravan   22\n",
      "student-2   2    harsha   22\n",
      "student-3   3   deepika   21\n",
      "student-4   4  jyothika   23\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Create JSON data with student details\n",
    "json_data = '''\n",
    "{\n",
    "    \"student-1\": { \"id\": \"1\", \"name\": \"sravan\",\"age\":22 },\n",
    "    \"student-2\":{ \"id\": \"2\", \"name\": \"harsha\",\"age\":22 },\n",
    "    \"student-3\": { \"id\": \"3\", \"name\": \"deepika\",\"age\":21 },\n",
    "    \"student-4\": { \"id\": \"4\", \"name\": \"jyothika\",\"age\":23 }\n",
    "} '''\n",
    "print(json_data)\n",
    "# Convert JSON file with index orient\n",
    "df = pd.read_json(  json_data,\n",
    "                    orient ='index')\n",
    "# Display the Dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 13:49:29.812 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\Krupesh\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "actions = {'A': st.write, 'B': st.write, 'C': st.write}\n",
    "choice = st.selectbox('Choose one:', ['_', 'A', 'B', 'C'])\n",
    "if choice != '_':\n",
    "    result = actions[choice](f'You chose {choice}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import os\n",
    "import math\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Set some constant variables, I could put all of this in a seperate config file\n",
    "ALPACA_API_KEY = 'tpHEpdlolUR703YTAdrvOHaUMW6PEDPw' #os.environ.get('ALPACA_API_KEY')\n",
    "START_DATE = '2005-01-03'\n",
    "END_DATE = '2020-10-23'\n",
    "# URL for all the tickers on Polygon\n",
    "# POLYGON_TICKERS_URL = 'https://api.polygon.io/v2/reference/tickers?page={}&apiKey={}'\n",
    "POLYGON_TICKERS_URL = 'https://api.polygon.io/v3/reference/tickers?active={}&sort=ticker&order={}&limit={}&apiKey={}'\n",
    "# URL FOR PRICING DATA - Note, getting pricing that is UNADJUSTED for splits, I will try and adjust those manually\n",
    "POLYGON_AGGS_URL = 'https://api.polygon.io/v2/aggs/ticker/{}/range/1/day/{}/{}?unadjusted=true&apiKey={}'\n",
    "# URL FOR DIVIDEND DATA\n",
    "POLYGON_DIV_URL = 'https://api.polygon.io/v2/reference/dividends/{}?apiKey={}'\n",
    "# URL FOR STOCK SPLITS\n",
    "POLYGON_SPLIT_URL = 'https://api.polygon.io/v2/reference/splits/{}?apiKey={}'\n",
    "#URL FOR TICKER TYPES\n",
    "POLYGON_TYPES_URL = 'https://api.polygon.io/v2/reference/types?apiKey={}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% \n",
    "# Get the list of all supported tickers from Polygon.io\n",
    "def get_tickers(url = POLYGON_TICKERS_URL):\n",
    "    page = 1\n",
    "    active = True\n",
    "    order = 'asc'\n",
    "    limit = 500\n",
    "\n",
    "    session = requests.Session()\n",
    "    print(POLYGON_TICKERS_URL.format(active,order,limit, ALPACA_API_KEY))\n",
    "    # Initial request to get the ticker count\n",
    "    r = session.get(POLYGON_TICKERS_URL.format(active,order,limit, ALPACA_API_KEY))\n",
    "    data = r.json()\n",
    "    # print(data)\n",
    "    # This is to figure out how many pages to run pagination \n",
    "    count = data['count']\n",
    "    print('total tickers ' + str(count))\n",
    "    \n",
    "    df = pd.DataFrame(data['results'])\n",
    "    df.to_csv('data/tickers/tickerlist.csv', index=False)\n",
    "\n",
    "    # pages = math.ceil(count / data['perPage'])\n",
    "\n",
    "    # # Pull in all the pages of tickers\n",
    "    # for pages in range (2, pages+1):  # For production\n",
    "    # # for pages in range (2, 10):  # For testing\n",
    "    #     r = session.get(POLYGON_TICKERS_URL.format(page, ALPACA_API_KEY))\n",
    "    #     data = r.json()\n",
    "    #     df = pd.DataFrame(data['tickers'])\n",
    "    #     df.to_csv('data/tickers/{}.csv'.format(page), index=False)\n",
    "    #     print('Page {} processed'.format(page))\n",
    "    #     page += 1\n",
    "        \n",
    "    return('Processes {} count of tickers'.format(count))\n",
    "\n",
    "\n",
    "# Stich all of these csv files into one dataframe for analysis\n",
    "def combine_tickers(directory):\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for f in os.listdir(directory):\n",
    "        df2 = pd.read_csv('{}/{}'.format(directory, f))\n",
    "        df = df.append(df2)\n",
    "    \n",
    "    # Read out a copy of the file to a csv for later analysis\n",
    "    df.set_index('ticker', inplace=True)\n",
    "    df.drop_duplicates()  # Just in case any tickers get pulled twice\n",
    "    df.to_csv('polygon_tickers.csv')\n",
    "    print(df)    \n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_us_exch(ticker_df):\n",
    "    \n",
    "    # Keep only U.S. Dollar denominated securities\n",
    "    df = ticker_df[(ticker_df.currency_name == 'USD') & (ticker_df.locale == 'US')]\n",
    "    # Keep only the primary U.S. exchanges\n",
    "    exch = ['AMX','ARCA','BATS','NASDAQ','NSC','NYE']\n",
    "    df = df[df['primary_exchange'].isin(exch)]\n",
    "    # Filter out preferred stock, american depositry receipts, closed end funds, reit\n",
    "    stockTypes = ['PFD','ADR','CEF','MLP','REIT','RIGHT','UNIT','WRT']\n",
    "    df = df[df['type'].isin(stockTypes) == False]\n",
    "    \n",
    "    df.to_csv('polygon_tickers_us.csv')\n",
    "\n",
    "    # Create a list of symbols to loop thru\n",
    "    symbols = df.index.tolist()\n",
    "\n",
    "    return symbols\n",
    "\n",
    "\n",
    "# Get the aggregated bars for the symbols I need\n",
    "def get_bars(symbolslist, outdir, start, end):\n",
    "\n",
    "    session = requests.Session()\n",
    "    # In case I run into issues, retry my connection\n",
    "    retries = Retry(total=5, backoff_factor=0.1, status_forcelist=[ 500, 502, 503, 504 ])\n",
    "\n",
    "    session.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "    count = 0\n",
    "    \n",
    "    barlog = open(\"barlog.txt\", \"w\")\n",
    "    \n",
    "    for symbol in symbolslist:\n",
    "        try:\n",
    "            r = session.get(POLYGON_AGGS_URL.format(symbol, start, end, ALPACA_API_KEY))\n",
    "            if r:\n",
    "                data = r.json()\n",
    "            \n",
    "                # create a pandas dataframe from the information\n",
    "                if data['queryCount'] > 1:\n",
    "                    df = pd.DataFrame(data['results'])\n",
    "                    df['date'] = pd.to_datetime(df['t'], unit='ms')\n",
    "                    df['date'] =  df['date'].dt.date.astype(str)\n",
    "                    df.set_index('date', inplace=True)\n",
    "                    df['symbol'] = symbol\n",
    "\n",
    "                    df.drop(columns=['vw', 't', 'n'], inplace=True)\n",
    "                    df.rename(columns={'v': 'volume', 'o': 'open', 'c': 'close', 'h': 'high', 'l': 'low'}, inplace=True)\n",
    "\n",
    "                    df.to_csv('{}/{}.csv'.format(outdir, symbol), index=True)\n",
    "                    count += 1\n",
    "\n",
    "                    # Logging, I could write a short method for this to reuse\n",
    "                    msg = (symbol + ' file created with record count ' + str(data['queryCount']))\n",
    "                    print(msg)\n",
    "                    barlog.write(msg)\n",
    "                    barlog.write(\"\\n\")\n",
    "\n",
    "                else:\n",
    "                    msg = ('No data for symbol ' + str(symbol))\n",
    "                    print(msg)\n",
    "                    barlog.write(msg)\n",
    "                    barlog.write(\"\\n\")\n",
    "            else:\n",
    "                msg = ('No response for symbol ' + str(symbol))\n",
    "                print(msg)\n",
    "                barlog.write(msg)\n",
    "                barlog.write(\"\\n\")\n",
    "        # Raise exception but continue           \n",
    "        except:\n",
    "            msg = ('****** exception raised for symbol ' + str(symbol))\n",
    "            print(msg)\n",
    "            barlog.write(msg)\n",
    "            barlog.write(\"\\n\")\n",
    "    \n",
    "    barlog.close()\n",
    "    return ('{} file were exported'.format(count))\n",
    "\n",
    "\n",
    "# Define a function to pull in the splits data\n",
    "def get_splits(symbolslist, outdir):\n",
    "\n",
    "    session = requests.Session()\n",
    "    # In case I run into issues, retry my connection\n",
    "    retries = Retry(total=5, backoff_factor=0.1, status_forcelist=[ 500, 502, 503, 504 ])\n",
    "\n",
    "    session.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "    count = 0\n",
    "    \n",
    "    # Get the split data\n",
    "    for symbol in symbolslist:\n",
    "        try:\n",
    "            r = session.get(POLYGON_SPLIT_URL.format(symbol, ALPACA_API_KEY))\n",
    "            if r:\n",
    "                data = r.json()\n",
    "                if data['count'] > 0:\n",
    "                    df = pd.DataFrame(data['results'])\n",
    "                    df.rename(columns={'exDate': 'date', 'declaredDate': 'splitDeclaredDate'}, inplace=True)\n",
    "                    df.drop(columns=['paymentDate'], inplace=True)\n",
    "                    df.set_index('date', inplace=True)\n",
    "                    df.to_csv('{}/{}.csv'.format(outdir, symbol), index=True)\n",
    "                    \n",
    "                    print('split file for ' + symbol + ' ' + str(data['count']))\n",
    "                    count += 1\n",
    "                else:\n",
    "                    print('No data for symbol ' + str(symbol))\n",
    "            else:\n",
    "                print('No response for symbol ' + str(symbol))\n",
    "        # Raise exception but continue           \n",
    "        except:\n",
    "            print('****** exception raised for symbol ' + str(symbol))\n",
    "            \n",
    "    return ('{} file were exported'.format(count))\n",
    "\n",
    "\n",
    "# Fix erroneous splits from a correction file manually created\n",
    "def fix_splits(splitpath):\n",
    "    # Get the split corrections to overwrite\n",
    "    correct_df = pd.read_csv('split_corrections.csv')\n",
    "    # create a list of symbols to fix\n",
    "    symbols = correct_df['ticker'].tolist()\n",
    "    # remove duplicates\n",
    "    symbols = list(dict.fromkeys(symbols))\n",
    "\n",
    "    # for symbol in symbols:\n",
    "    for symbol in symbols:\n",
    "        print(symbol)\n",
    "\n",
    "    # get any splits\n",
    "        if os.path.isfile('{}/{}.csv'.format(splitpath, symbol)):\n",
    "            df = pd.read_csv('{}/{}.csv'.format(splitpath, symbol))\n",
    "            print(df)\n",
    "            df = pd.merge(df, correct_df, how='left', left_on=['date', 'ticker'], right_on=['date', 'ticker'])\n",
    "            \n",
    "            for index, row in df.iterrows():\n",
    "                # Adjust bad dates\n",
    "                if not pd.isnull(row.date_adj):\n",
    "                    df.loc[index, 'date'] = row.date_adj\n",
    "                # Adjust bad ratios\n",
    "                if not pd.isnull(row.ratio_adj):\n",
    "                    df.loc[index, 'ratio'] = row.ratio_adj\n",
    "                else:\n",
    "                    df.loc[index, 'ratio'] = row.ratio_x\n",
    "            \n",
    "            # Format the dataframe for export\n",
    "            df = df[['date', 'ticker', 'ratio']]\n",
    "            df.set_index('date', inplace=True)\n",
    "            print(df)\n",
    "\n",
    "            # Overwrite the file with this new file\n",
    "            df.to_csv('{}/{}.csv'.format(splitpath, symbol))\n",
    "            print('Split file for {} corrected'.format(symbol))\n",
    "            \n",
    "        else:\n",
    "            print('no file found')\n",
    "                \n",
    "    return ('Split file corrections complete')\n",
    "\n",
    "\n",
    "# Define a function to pull in the splits data\n",
    "def get_divs(symbolslist, outdir):\n",
    "\n",
    "    session = requests.Session()\n",
    "    count = 0\n",
    "    \n",
    "    # Get the split data\n",
    "    for symbol in symbolslist: # ['AAPL']:\n",
    "        r = session.get(POLYGON_DIV_URL.format(symbol, ALPACA_API_KEY))\n",
    "        data = r.json()\n",
    "        if data['count'] > 0:\n",
    "            df = pd.DataFrame(data['results'])\n",
    "            # df.rename(columns={'paymentDate': 'date'}, inplace=True)\n",
    "            df.rename(columns={'exDate': 'date', 'amount': 'dividend',\n",
    "                               'paymentDate': 'divPaymentDate',\n",
    "                               'recordDate': 'divRecordDate',\n",
    "                               'declaredDate': 'divDeclaredDate'}, inplace=True)\n",
    "            df.set_index('date', inplace=True)\n",
    "            df = df.groupby(df.index).first()\n",
    "            df.to_csv('{}/{}.csv'.format(outdir, symbol), index=True)\n",
    "            \n",
    "            print('div file for ' + symbol + ' ' + str(data['count']))\n",
    "            count += 1\n",
    "            \n",
    "    return ('{} file were exported'.format(count))\n",
    "\n",
    "\n",
    "# Combine bars, splits and dividend\n",
    "def combine_bars(barpath, splitpath, divpath):\n",
    "\n",
    "    count = 0\n",
    "    for f in os.listdir(barpath):\n",
    "        \n",
    "        symbol = f[:-4]\n",
    "        print(symbol)\n",
    "        \n",
    "        # Get the bar data\n",
    "        if os.path.isfile('{}/{}.csv'.format(barpath, symbol)):\n",
    "            bars = pd.read_csv('{}/{}.csv'.format(barpath, symbol), index_col='date')\n",
    "            \n",
    "            # get any splits\n",
    "            if os.path.isfile('{}/{}.csv'.format(splitpath, symbol)):\n",
    "                splits = pd.read_csv('{}/{}.csv'.format(splitpath, symbol), index_col='date')\n",
    "                splits.drop(columns=['ticker'], inplace=True)\n",
    "                \n",
    "                bars = bars.merge(splits, left_index=True, right_index=True, how='left')\n",
    "\n",
    "            else:\n",
    "                \n",
    "                bars = bars\n",
    "            \n",
    "            # get any dividend payments\n",
    "            if os.path.isfile('{}/{}.csv'.format(divpath, symbol)):\n",
    "                divs = pd.read_csv('{}/{}.csv'.format(divpath, symbol), index_col='date')\n",
    "                divs.drop(columns=['ticker'], inplace=True)\n",
    "            \n",
    "                bars = bars.merge(divs, left_index=True, right_index=True, how='left')\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                bars = bars\n",
    "                \n",
    "            # Export bars \n",
    "            bars.to_csv('data/bars_adj/{}.csv'.format(symbol))\n",
    "            count += 1\n",
    "        \n",
    "    return ('{} adjusted bar file were exported'.format(count))\n",
    "\n",
    "\n",
    "# Adjust the OHLCV data for stock splits\n",
    "def adj_bars(directory):\n",
    "\n",
    "    count = 0\n",
    "    for f in os.listdir(directory):\n",
    "\n",
    "        df = pd.read_csv('{}/{}'.format(directory, f), index_col='date')\n",
    "        \n",
    "        if 'ratio' in df.columns:\n",
    "            df['ratio_adj'] = df['ratio']\n",
    "        else:\n",
    "             df['ratio_adj'] = 1\n",
    "\n",
    "        # Create a split factor, shifted to the day earlier.  Also, fill in any missing factors with 1\n",
    "        df['split_factor'] = (1 / df['ratio_adj'].shift(-1)).fillna(1)\n",
    "        #  Create a cumulative product of the splits, in reverse order using the []::-1]\n",
    "        df['split_factor'] = df['split_factor'][::-1].cumprod()\n",
    "\n",
    "        # Adjust the various OHLCV metrics\n",
    "        df['volume_adj'] = df['volume'] * df['split_factor']\n",
    "        df['open_adj'] = df['open'] / df['split_factor']\n",
    "        df['close_adj'] = df['close'] / df['split_factor']\n",
    "        df['high_adj'] = df['high'] / df['split_factor']\n",
    "        df['low_adj'] = df['low'] / df['split_factor']\n",
    "        df['dollar_volume'] = df['volume'] * df['close']\n",
    "\n",
    "        df.to_csv('{}/{}'.format(directory, f))\n",
    "        count += 1\n",
    "        \n",
    "    return ('{} files was adjusted'.format(count))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.polygon.io/v3/reference/tickers?active=True&sort=ticker&order=asc&limit=500&apiKey=tpHEpdlolUR703YTAdrvOHaUMW6PEDPw\n",
      "total tickers 500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Processes 500 count of tickers'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%  Get all the tickers on Polygon.io and save them to a data directory\n",
    "get_tickers()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     name  market locale  \\\n",
      "ticker                                                                     \n",
      "A                               Agilent Technologies Inc.  stocks     us   \n",
      "AA                                      Alcoa Corporation  stocks     us   \n",
      "AAA                       AAF First Priority CLO Bond ETF  stocks     us   \n",
      "AAAU               Goldman Sachs Physical Gold ETF Shares  stocks     us   \n",
      "AAC                          Ares Acquisition Corporation  stocks     us   \n",
      "...                                                   ...     ...    ...   \n",
      "ALX                                      Alexander's Inc.  stocks     us   \n",
      "ALXO              ALX Oncology Holdings Inc. Common Stock  stocks     us   \n",
      "ALYA    Alithya Group inc. Class A Subordinate Voting ...  stocks     us   \n",
      "ALZN                    Alzamend Neuro, Inc. Common Stock  stocks     us   \n",
      "ALpA    Air Lease Corporation 6.150% Fixed-to-Floating...  stocks     us   \n",
      "\n",
      "       primary_exchange type  active currency_name        cik composite_figi  \\\n",
      "ticker                                                                         \n",
      "A                  XNYS   CS    True           usd  1090872.0   BBG000C2V3D6   \n",
      "AA                 XNYS   CS    True           usd  1675149.0   BBG00B3T3HD3   \n",
      "AAA                ARCX  ETF    True           usd        NaN   BBG00X5FSP48   \n",
      "AAAU               BATS  ETF    True           usd  1708646.0   BBG00LPXX872   \n",
      "AAC                XNYS   CS    True           usd  1829432.0            NaN   \n",
      "...                 ...  ...     ...           ...        ...            ...   \n",
      "ALX                XNYS   CS    True           usd     3499.0   BBG000BBNLK8   \n",
      "ALXO               XNAS   CS    True           usd  1810182.0   BBG00VR8SDG8   \n",
      "ALYA               XNAS   CS    True           usd  1734520.0   BBG00MFNPFZ5   \n",
      "ALZN               XNAS   CS    True           usd  1677077.0   BBG00ZMXG7M8   \n",
      "ALpA               XNYS  PFD    True           usd  1487712.0            NaN   \n",
      "\n",
      "       share_class_figi      last_updated_utc  \n",
      "ticker                                         \n",
      "A          BBG001SCTQY4  2022-05-05T00:00:00Z  \n",
      "AA         BBG00B3T3HF1  2022-05-05T00:00:00Z  \n",
      "AAA        BBG00X5FSPZ4  2022-05-05T00:00:00Z  \n",
      "AAAU       BBG00LPXX8Z1  2022-05-05T00:00:00Z  \n",
      "AAC                 NaN  2022-05-05T00:00:00Z  \n",
      "...                 ...                   ...  \n",
      "ALX        BBG001S5NM92  2022-05-05T00:00:00Z  \n",
      "ALXO       BBG00VR8SF73  2022-05-05T00:00:00Z  \n",
      "ALYA       BBG00MFNPGQ3  2022-05-05T00:00:00Z  \n",
      "ALZN       BBG00ZMXG7N7  2022-05-05T00:00:00Z  \n",
      "ALpA                NaN  2022-05-05T00:00:00Z  \n",
      "\n",
      "[500 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "#%% Combine all the paginated ticker files together into one dataframe\n",
    "symbols = combine_tickers('data/tickers')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%  Filter down to the tickers I'm interestead in (this could also be done by modifying get_tickers)\n",
    "symbols = filter_us_exch(symbols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0 file were exported'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%% Get all the aggregated bar/pricing data for each symbol in the filtered list\n",
    "get_bars(symbols, 'data/bars', START_DATE, END_DATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'split_corrections.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mg:\\SPU\\620\\Week9\\simple_streamlit.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/SPU/620/Week9/simple_streamlit.ipynb#ch0000008?line=0'>1</a>\u001b[0m get_splits(symbols, \u001b[39m'\u001b[39m\u001b[39mdata/splits\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/SPU/620/Week9/simple_streamlit.ipynb#ch0000008?line=1'>2</a>\u001b[0m \u001b[39m# Fix data for about 50 splits from a correction file created manually\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/SPU/620/Week9/simple_streamlit.ipynb#ch0000008?line=2'>3</a>\u001b[0m fix_splits(\u001b[39m'\u001b[39;49m\u001b[39mdata/splits\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32mg:\\SPU\\620\\Week9\\simple_streamlit.ipynb Cell 4'\u001b[0m in \u001b[0;36mfix_splits\u001b[1;34m(splitpath)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/g%3A/SPU/620/Week9/simple_streamlit.ipynb#ch0000003?line=168'>169</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfix_splits\u001b[39m(splitpath):\n\u001b[0;32m    <a href='vscode-notebook-cell:/g%3A/SPU/620/Week9/simple_streamlit.ipynb#ch0000003?line=169'>170</a>\u001b[0m     \u001b[39m# Get the split corrections to overwrite\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/g%3A/SPU/620/Week9/simple_streamlit.ipynb#ch0000003?line=170'>171</a>\u001b[0m     correct_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39msplit_corrections.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/g%3A/SPU/620/Week9/simple_streamlit.ipynb#ch0000003?line=171'>172</a>\u001b[0m     \u001b[39m# create a list of symbols to fix\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/g%3A/SPU/620/Week9/simple_streamlit.ipynb#ch0000003?line=172'>173</a>\u001b[0m     symbols \u001b[39m=\u001b[39m correct_df[\u001b[39m'\u001b[39m\u001b[39mticker\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/util/_decorators.py?line=304'>305</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/util/_decorators.py?line=305'>306</a>\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/util/_decorators.py?line=306'>307</a>\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/util/_decorators.py?line=307'>308</a>\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/util/_decorators.py?line=308'>309</a>\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/util/_decorators.py?line=309'>310</a>\u001b[0m     )\n\u001b[1;32m--> <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/util/_decorators.py?line=310'>311</a>\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:586\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/readers.py?line=570'>571</a>\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/readers.py?line=571'>572</a>\u001b[0m     dialect,\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/readers.py?line=572'>573</a>\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/readers.py?line=581'>582</a>\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/readers.py?line=582'>583</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/readers.py?line=583'>584</a>\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/readers.py?line=585'>586</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:482\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/readers.py?line=478'>479</a>\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/readers.py?line=480'>481</a>\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/readers.py?line=481'>482</a>\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/readers.py?line=483'>484</a>\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/readers.py?line=484'>485</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:811\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/readers.py?line=807'>808</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwds:\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/readers.py?line=808'>809</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m--> <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/readers.py?line=810'>811</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:1040\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/readers.py?line=1035'>1036</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/readers.py?line=1036'>1037</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown engine: \u001b[39m\u001b[39m{\u001b[39;00mengine\u001b[39m}\u001b[39;00m\u001b[39m (valid options are \u001b[39m\u001b[39m{\u001b[39;00mmapping\u001b[39m.\u001b[39mkeys()\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/readers.py?line=1037'>1038</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/readers.py?line=1038'>1039</a>\u001b[0m \u001b[39m# error: Too many arguments for \"ParserBase\"\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/readers.py?line=1039'>1040</a>\u001b[0m \u001b[39mreturn\u001b[39;00m mapping[engine](\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:51\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/c_parser_wrapper.py?line=47'>48</a>\u001b[0m kwds[\u001b[39m\"\u001b[39m\u001b[39musecols\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39musecols\n\u001b[0;32m     <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/c_parser_wrapper.py?line=49'>50</a>\u001b[0m \u001b[39m# open handles\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/c_parser_wrapper.py?line=50'>51</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open_handles(src, kwds)\n\u001b[0;32m     <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/c_parser_wrapper.py?line=51'>52</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/c_parser_wrapper.py?line=53'>54</a>\u001b[0m \u001b[39m# Have to pass int, would break tests using TextReader directly otherwise :(\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\base_parser.py:222\u001b[0m, in \u001b[0;36mParserBase._open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/base_parser.py?line=217'>218</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_handles\u001b[39m(\u001b[39mself\u001b[39m, src: FilePathOrBuffer, kwds: \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, Any]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/base_parser.py?line=218'>219</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/base_parser.py?line=219'>220</a>\u001b[0m \u001b[39m    Let the readers open IOHandles after they are done with their potential raises.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/base_parser.py?line=220'>221</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/base_parser.py?line=221'>222</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/base_parser.py?line=222'>223</a>\u001b[0m         src,\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/base_parser.py?line=223'>224</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/base_parser.py?line=224'>225</a>\u001b[0m         encoding\u001b[39m=\u001b[39;49mkwds\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/base_parser.py?line=225'>226</a>\u001b[0m         compression\u001b[39m=\u001b[39;49mkwds\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/base_parser.py?line=226'>227</a>\u001b[0m         memory_map\u001b[39m=\u001b[39;49mkwds\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/base_parser.py?line=227'>228</a>\u001b[0m         storage_options\u001b[39m=\u001b[39;49mkwds\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/base_parser.py?line=228'>229</a>\u001b[0m         errors\u001b[39m=\u001b[39;49mkwds\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/parsers/base_parser.py?line=229'>230</a>\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\common.py:702\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/common.py?line=696'>697</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/common.py?line=697'>698</a>\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/common.py?line=698'>699</a>\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/common.py?line=699'>700</a>\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/common.py?line=700'>701</a>\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/common.py?line=701'>702</a>\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/common.py?line=702'>703</a>\u001b[0m             handle,\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/common.py?line=703'>704</a>\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/common.py?line=704'>705</a>\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/common.py?line=705'>706</a>\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/common.py?line=706'>707</a>\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/common.py?line=707'>708</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/common.py?line=708'>709</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/common.py?line=709'>710</a>\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Krupesh/AppData/Roaming/Python/Python310/site-packages/pandas/io/common.py?line=710'>711</a>\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'split_corrections.csv'"
     ]
    }
   ],
   "source": [
    "get_splits(symbols, 'data/splits')\n",
    "# Fix data for about 50 splits from a correction file created manually\n",
    "fix_splits('data/splits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%  Pull in all the dividend data\n",
    "get_divs(symbols, 'data/divs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%  Combine the bars (pricing data) with any splits and dividend payments\n",
    "combine_bars('data/bars', 'data/splits', 'data/divs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%  Create new and stock split adjusted OHLCV fields\n",
    "adj_bars('data/bars_adj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "bars = pd.read_csv('data/bars_adj/AAPL.csv')\n",
    "bars['close'].plot()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b89b5cfaba6639976dc87ff2fec6d58faec662063367e2c229c520fe71072417"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
